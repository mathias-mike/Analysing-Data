{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze A/B Test Results\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists. For this project, we will be working to understand the results of an A/B test run by an e-commerce website.  Our goal is to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "To get started, let's import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to assure you get the same answers on quizzes as we set up\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the `ab_data.csv` data and store it in `df`. Below is the description of the data, there are a total of 5 columns:\n",
    "\n",
    "<center>\n",
    "\n",
    "|Data columns|Purpose|Valid values|\n",
    "| ------------- |:-------------| -----:|\n",
    "|user_id|Unique ID|Int64 values|\n",
    "|timestamp|Time stamp when the user visited the webpage|-|\n",
    "|group|In the current A/B experiment, the users are categorized into two broad groups. <br>The `control` group users are expected to be served with `old_page`; and `treatment` group users are matched with the `new_page`. <br>However, **some inaccurate rows** are present in the initial data, such as a `control` group user is matched with a `new_page`. |`['control', 'treatment']`|\n",
    "|landing_page|It denotes whether the user visited the old or new webpage.|`['old_page', 'new_page']`|\n",
    "|converted|It denotes whether the user decided to pay for the company's product. Here, `1` means yes, the user bought the product.|`[0, 1]`|\n",
    "</center>\n",
    "Use your dataframe to answer the questions in Quiz 1 of the classroom.\n",
    "\n",
    "\n",
    "**a.** Read in the dataset from the `ab_data.csv` file and take a look at the top few rows here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      "user_id         294478 non-null int64\n",
      "timestamp       294478 non-null object\n",
      "group           294478 non-null object\n",
      "landing_page    294478 non-null object\n",
      "converted       294478 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Use the cell below to find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294478, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**d.** The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965919355605512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**e.** The number of times when the \"group\" is `treatment` but \"landing_page\" is not a `new_page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('group == \"treatment\" and landing_page != \"new_page\"').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "old_page    147239\n",
       "new_page    147239\n",
       "Name: landing_page, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.landing_page.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f.** Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      "user_id         294478 non-null int64\n",
      "timestamp       294478 non-null object\n",
      "group           294478 non-null object\n",
      "landing_page    294478 non-null object\n",
      "converted       294478 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Non has missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Proportion of `control` and `treatment` group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49987435394155083"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['group'] == \"control\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50012564605844922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['group'] == \"treatment\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment    147276\n",
       "control      147202\n",
       "Name: group, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 1.2  \n",
    "In a particular row, the **group** and **landing_page** columns should have either of the following acceptable values:\n",
    "\n",
    "|user_id| timestamp|group|landing_page|converted|\n",
    "|---|---|---|---|---|\n",
    "|XXXX|XXXX|`control`| `old_page`|X |\n",
    "|XXXX|XXXX|`treatment`|`new_page`|X |\n",
    "\n",
    "\n",
    "It means, the `control` group users should match with `old_page`; and `treatment` group users should matched with the `new_page`. \n",
    "\n",
    "However, for the rows where `treatment` does not match with `new_page` or `control` does not match with `old_page`, we cannot be sure if such rows truly received the new or old wepage.  \n",
    "\n",
    "\n",
    "Use **Quiz 2** in the classroom to figure out how should we handle the rows where the group and landing_page columns don't match?\n",
    "\n",
    "**a.** Now use the answer to the quiz to create a new dataset that meets the specifications from the quiz.  Store your new dataframe in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the inaccurate rows, and store the result in a new dataframe df2\n",
    "df2 = df.query('(group == \"control\" and landing_page == \"old_page\") or (group == \"treatment\" and landing_page == \"new_page\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the incorrect rows were removed from df2 - \n",
    "# Output of the statement below should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 1.3  \n",
    "Use **df2** and the cells below to answer questions for **Quiz3** in the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**b.** There is one **user_id** repeated in **df2**.  What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290585, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.user_id.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Display the rows for the duplicate **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.query('user_id == 773192')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Remove **one** of the rows with a duplicate **user_id**, from the **df2** dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, timestamp, group, landing_page, converted]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove one of the rows with a duplicate user_id..\n",
    "# Hint: The dataframe.drop_duplicates() may not work in this case because the rows with duplicate user_id are not entirely identical. \n",
    "df2 = df2[~df2.user_id.duplicated()]\n",
    "\n",
    "# Check again if the row with a duplicate user_id is deleted or not\n",
    "df2[df2.user_id.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 1.4  \n",
    "Use **df2** in the cells below to answer the quiz questions related to **Quiz 4** in the classroom.\n",
    "\n",
    "**a.** What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Population conversion probability regardless of page\n",
    "p_population = df2['converted'].mean() \n",
    "p_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion probability for the control group\n",
    "p_control = df2.query('group == \"control\"')['converted'].mean() \n",
    "p_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion probability for the treatment group\n",
    "p_treatment = df2.query('group == \"treatment\"')['converted'].mean() \n",
    "p_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observed difference in probability between treatment and control group\n",
    "obs_diff = p_treatment - p_control\n",
    "obs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50006194422266881"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of user being assigned to the new_page\n",
    "p_new_page = (df2['landing_page'] == \"new_page\").mean()\n",
    "p_new_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** Consider your results from parts (a) through (d) above, and explain below whether the new `treatment` group users lead to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">With the `landing_page` split into halve for `new_page` and `old_page` and the\n",
    "probability of convertion in the `old_page` being larger than that of the `new_page` by `0.00158`, I would say the new `treatment` group leads to less conversion than the `control` treatment group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "`1.` For now, consider you need to make the decision just based on all the data provided.  If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should your null and alternative hypotheses be?  You can state your hypothesis in terms of words or in terms of **$p_{old}$** and **$p_{new}$**, which are the converted rates for the old and new pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_0: p_{new} - p_{old} \\leq 0  \\\\\n",
    "H_1: p_{new} - p_{old} > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "Use a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Perform the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>\n",
    "\n",
    "Use the cells below to provide the necessary parts of this simulation.  If this doesn't make complete sense right now, don't worry - you are going to work through the problems below to complete this problem.  You can use **Quiz 5** in the classroom to make sure you are on the right track.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the **conversion rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The probability of conversion for the new_page. Under null, we assume p_new = converted success rate regardless of page. \n",
    "p_new = df2['converted'].mean()\n",
    "p_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the **conversion rate** for $p_{old}$ under the null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The probability of conversion for the old_page. Under null, p_new = p_old\n",
    "p_old = p_new\n",
    "p_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is $n_{new}$, the number of individuals in the treatment group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of users in treatment group who received new_page\n",
    "n_new = df2.query('group == \"treatment\"').shape[0]\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is $n_{old}$, the number of individuals in the control group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of users in control group who received old_page\n",
    "n_old = df2.query('group == \"control\"').shape[0]\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate $n_{new}$ transactions with a conversion rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A trial simulation\n",
    "new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new])\n",
    "new_page_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate $n_{old}$ transactions with a conversion rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial simulation\n",
    "old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_old, p_old])\n",
    "old_page_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find $p_{new}$ - $p_{old}$ for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010165911397400917"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability difference in trial simulation\n",
    "new_page_converted.mean() - old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Create 10,000 $p_{new}$ - $p_{old}$ values using the same simulation process you used in parts (a) through (g) above. Store all 10,000 values in a NumPy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above method used in the trial simulation works pretty fine but is slow. Using np.random.binomial() does\n",
    "# this alot faster since it uses numpy based operations.\n",
    "\n",
    "# new_page conversion probability simulated 10,000 times. \n",
    "# np.random.binomial() returns the number of successes over n_new trails so we need to divide by n_new to get the proportion\n",
    "new_page_conversion_simulation = np.random.binomial(n_new, p_new, 10000)/n_new\n",
    "\n",
    "# old_page conversion probability simulated 10,000 times\n",
    "# Same applies for dividing by n_old\n",
    "old_page_conversion_simulation = np.random.binomial(n_old, p_old, 10000)/n_old\n",
    "\n",
    "# Conversion probability difference between new_page and old_page for 10,000 simulations\n",
    "p_diffs = new_page_conversion_simulation - old_page_conversion_simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot a histogram of the **p_diffs**.  Does this plot look like what you expected?  Use the matching problem in the classroom to assure you fully understand what was computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011923301594755017"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard deviation for the distribution of difference in mean obtained from the above simulation\n",
    "p_diff_std = p_diffs.std()\n",
    "p_diff_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda3df9ccf8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting histogram of our difference in mean distribution\n",
    "plt.hist(p_diffs);\n",
    "plt.xlabel('differences');\n",
    "plt.ylabel('count');\n",
    "\n",
    "# Plotting our observed difference in mean with the black color\n",
    "plt.axvline(x=obs_diff, color='black', label='obs_diff');\n",
    "\n",
    "# Plotting 1 standard deviation away from the mean in yellow\n",
    "high_1_std = p_diffs.mean() + p_diff_std\n",
    "low_1_std = p_diffs.mean() - p_diff_std\n",
    "plt.axvline(x=high_1_std, color='yellow', label='1 std');\n",
    "plt.axvline(x=low_1_std, color='yellow');\n",
    "\n",
    "# Plotting 2 standard deviation away from the mean in red\n",
    "high_2_std = p_diffs.mean() + (2 * p_diff_std)\n",
    "low_2_std = p_diffs.mean() - (2 * p_diff_std)\n",
    "plt.axvline(x=high_2_std, color='red', label='2 std');\n",
    "plt.axvline(x=low_2_std, color='red');\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90710000000000002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining our p_value from our bootstrapped sampling distribution\n",
    "p_value = (p_diffs > obs_diff).mean()\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. Please explain using the vocabulary you've learned in this course what you just computed in part **j.**  What is this value called in scientific studies?  What does this value mean in terms of whether or not there is a difference between the new and old pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This is the probability of obtaining our observed statistics, or a value more extreem in favor of the Alternate hypothesis ($H_1$) from the distribution we believe to be true under the Null hypothesis ($H_0$).\n",
    ">\n",
    ">Another way to state it is; If $H_0$ is true, the probability of obtaining our observed statistics or one more extreem in favour or the $H_1$.\n",
    ">\n",
    ">This is called the ***p_value*** in scientific studies.\n",
    ">\n",
    ">The ***p_value*** is high, higher than our **5% Type I error rate ($\\alpha$)**, which shows a high probability of our `obs_diff` coming from this distribution.\n",
    ">\n",
    ">From this, we would fail to reject the Null ($H_0$) and we believe the `old_page` leads to equal or more conversions than the `new_page`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We could also use a built-in to achieve similar results.  Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance. Fill in the below to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17489, 17264, 145274, 145310)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = df2.query('landing_page == \"old_page\" and converted == 1').shape[0]\n",
    "convert_new = df2.query('landing_page == \"new_page\" and converted == 1').shape[0]\n",
    "n_old = df2.query('landing_page == \"old_page\"').shape[0]\n",
    "n_new = df2.query('landing_page == \"new_page\"').shape[0]\n",
    "convert_old, convert_new, n_old, n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Now use `stats.proportions_ztest` to compute your test statistic and p-value.  [Here](https://docs.w3cub.com/statsmodels/generated/statsmodels.stats.proportion.proportions_ztest/) is a helpful link on using the built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3109241984234394, 0.90505831275902449)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">`z_score` is the number of standard deviation a point of observation is from the mean. Here we can see our point of observation (the `obs_diff`) is about `1.31` standand deviation away from the mean. \n",
    ">\n",
    ">You can also observe this from the histogram above; The `obs_diff` (in `black line`) is about `1.31` standard deviation below the mean. For a **one tailed test with 95% confidence level**, a `z-score` that does not exceed the **z-critical** value (`1.65`) does not prove the unlikeliness of our null, hence we fail to reject null.\n",
    ">\n",
    ">The `p_value` and an observation of the histogram above tells us that there is a `90.5%` probability of obtaining our statistics or a value more extreem in favour of $H_1$ if $H_0$ is true. \n",
    ">\n",
    ">Based on this, we agree with our finding in **j.** and **k.** above and **fail to reject the null hypothesis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` In this final part, you will see that the result you achieved in the A/B test in Part II above can also be achieved by performing regression.<br><br> \n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should you be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We would be performing a ***Logistic Regression*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use **statsmodels** to fit the regression model you specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives. However, you first need to create in df2 a column for the intercept, and create a dummy variable column for which page each user received.  Add an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   ab_page  intercept  \n",
       "0        0          1  \n",
       "1        0          1  \n",
       "2        1          1  \n",
       "3        1          1  \n",
       "4        0          1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']\n",
    "df2['intercept'] = 1\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to instantiate your regression model on the two columns you created in part b., then fit the model using the two columns you created in part **b.** to predict whether or not an individual converts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "logist_model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "result = logist_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of your model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-09-23 15:18</td>       <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-09-23 15:18 AIC:              212780.3502\n",
       "No. Observations:   290584           BIC:              212801.5095\n",
       "Df Model:           1                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290582           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9888    0.0081  -246.6690  0.0000  -2.0046  -1.9730\n",
       "ab_page      -0.0150    0.0114    -1.3109  0.1899  -0.0374   0.0074\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98511193960306265"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.0150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The `p_value` associated with the **independent variable**, `ab_page`, provides information on how statistically significant `ab_page` is in predicting the **response variable**, `conversion`. This `p_value` is related to a hypothesis test of whether the population slope is equal to `0` vs. an alternative when the population slope is not equal to `0`.\n",
    ">$$\n",
    "H_0: \\beta_1 = 0 \\\\\n",
    "H_1: \\beta_1 \\neq 0\n",
    "$$\n",
    ">\n",
    ">If our `p_value` $\\leq \\alpha$ and we **reject $H_0$**, it means that the `population slope` of out `independent variable` is different than `0` which gives statistical evidence of a **linear relationship** between the `independent variable` and the `dependent variable`. This in turn suggest that the `independent variable` is helpful in predicting the `dependent variable`. \n",
    ">\n",
    ">This is a **two-sided test** which is different from the **one-sided test** we ran in **Part II**. And with the two-sided test, we still fail to rejuct null because a `p_value` `{0.1899} > {0.05}` ($\\alpha$ level) shows no statistical evidence of `ab_page` being very useful in predicting `conversion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3109241984234394, 0.18988337448195103)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining a z_score and p_value for two tailed test using the same process as we did in Part II\n",
    "z_score2, p_value2 = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='two-sided')\n",
    "z_score2, p_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We get the same `p_value` when we run a **two-sided test** with the method in **Part II**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, you are considering other things that might influence whether or not an individual converts.  Discuss why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">It is a good idea to consider other factors to add into our regression model because in practice, there is often more then one factors that influence our response. Building a model based on one `predictor variable` completely ignores all other `predictor variables` that might influence our `response variable`. So in practice we want to build a model with all the `predictor variables` that a actually useful in predicting the `response variable`.\n",
    ">\n",
    ">However, when we do this, we need to take note of the word ***\"useful\"***. A couple of problems arise when we add more predictor variables to our models, one of this is **Multicolinearity**.\n",
    ">\n",
    ">**Multicolinearity** is a situation when the predictor variables are correlated with each other. In this case, some predictor variable are surrogates to others.\n",
    ">\n",
    ">In our case, we have seen that `ab_page` is not so useful in predicting our response, so we want to consider other variables that might be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check whether the time of the day of visit leads to more conversion.\n",
    "* First extract time category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131228    2017-01-02 13:42:05.378582\n",
      "184884    2017-01-02 13:42:15.234051\n",
      "Name: timestamp, dtype: object\n",
      "\n",
      "179072    2017-01-24 13:41:52.604673\n",
      "193652    2017-01-24 13:41:54.460509\n",
      "Name: timestamp, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Figuring out the duration of the experiment\n",
    "sorted_timestamp = df2.timestamp.sort_values()\n",
    "print(str(sorted_timestamp.head(2)) + '\\n\\n' + str(sorted_timestamp.tail(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We observe that the experiment was conducted within a month (the month of January 2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of the timestamp column from 'string' to 'datetime' for ease of handling.\n",
    "df2.timestamp = pd.to_datetime(df2.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            night\n",
      "1     late_morning\n",
      "2        afternoon\n",
      "3            night\n",
      "4    early_morning\n",
      "dtype: object\n",
      "\n",
      "294473            night\n",
      "294474    early_morning\n",
      "294475     late_morning\n",
      "294476    early_morning\n",
      "294477        afternoon\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's split our time values into 4 categories;\n",
    "# * Early Morning [00:00:00 - 06:00:00)\n",
    "# * Late Morning [06:00:00 - 12:00:00)\n",
    "# * Afternoon [12:00:00 - 18:00:00)\n",
    "# * Night [18:00:00 - 00:00:00)\n",
    "\n",
    "time_category = {} # Dictionary to hold the time category and it's index in the series\n",
    "\n",
    "for index, _time in df2.timestamp.dt.time.items():\n",
    "    if _time < datetime.time(6, 0, 0):\n",
    "        time_category[index] = 'early_morning'\n",
    "    elif _time < datetime.time(12, 0, 0):\n",
    "        time_category[index] = 'late_morning'\n",
    "    elif _time < datetime.time(18, 0, 0):\n",
    "        time_category[index] = 'afternoon'\n",
    "    else:\n",
    "        time_category[index] = 'night'\n",
    "        \n",
    "time_category = pd.Series(time_category)\n",
    "print(str(time_category.head()) + '\\n\\n' + str(time_category.tail()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>intercept</th>\n",
       "      <th>time_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>late_morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>early_morning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  timestamp      group landing_page  converted  \\\n",
       "0   851104 2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228 2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590 2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541 2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975 2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   ab_page  intercept  time_category  \n",
       "0        0          1          night  \n",
       "1        0          1   late_morning  \n",
       "2        1          1      afternoon  \n",
       "3        1          1          night  \n",
       "4        0          1  early_morning  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join our \"time_cat\" series to df2\n",
    "df2['time_category'] = time_category\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next build model with just time category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>intercept</th>\n",
       "      <th>time_category</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>early_morning</th>\n",
       "      <th>late_morning</th>\n",
       "      <th>night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>late_morning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>early_morning</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  timestamp      group landing_page  converted  \\\n",
       "0   851104 2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228 2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590 2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541 2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975 2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   ab_page  intercept  time_category  afternoon  early_morning  late_morning  \\\n",
       "0        0          1          night          0              0             0   \n",
       "1        0          1   late_morning          0              0             1   \n",
       "2        1          1      afternoon          1              0             0   \n",
       "3        1          1          night          0              0             0   \n",
       "4        0          1  early_morning          0              1             0   \n",
       "\n",
       "   night  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get time_category dummies\n",
    "df2 = df2.join(pd.get_dummies(df2['time_category']))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366103\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-09-23 15:18</td>       <td>AIC:</td>        <td>212775.4512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212817.7698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>3</td>         <td>Log-Likelihood:</td>  <td>-1.0638e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290580</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>        <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>    <td>-2.0286</td>  <td>0.0116</td>  <td>-175.1886</td> <td>0.0000</td> <td>-2.0513</td> <td>-2.0059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>late_morning</th> <td>0.0407</td>   <td>0.0163</td>   <td>2.5052</td>   <td>0.0122</td> <td>0.0089</td>  <td>0.0726</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>afternoon</th>    <td>0.0413</td>   <td>0.0162</td>   <td>2.5439</td>   <td>0.0110</td> <td>0.0095</td>  <td>0.0731</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>night</th>        <td>0.0467</td>   <td>0.0162</td>   <td>2.8792</td>   <td>0.0040</td> <td>0.0149</td>  <td>0.0785</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-09-23 15:18 AIC:              212775.4512\n",
       "No. Observations:   290584           BIC:              212817.7698\n",
       "Df Model:           3                Log-Likelihood:   -1.0638e+05\n",
       "Df Residuals:       290580           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "------------------------------------------------------------------\n",
       "                  Coef.  Std.Err.     z     P>|z|   [0.025  0.975]\n",
       "------------------------------------------------------------------\n",
       "intercept        -2.0286   0.0116 -175.1886 0.0000 -2.0513 -2.0059\n",
       "late_morning      0.0407   0.0163    2.5052 0.0122  0.0089  0.0726\n",
       "afternoon         0.0413   0.0162    2.5439 0.0110  0.0095  0.0731\n",
       "night             0.0467   0.0162    2.8792 0.0040  0.0149  0.0785\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model with early_morning as baseline\n",
    "logist_mod2 = sm.Logit(df2['converted'], df2[['intercept', 'late_morning', 'afternoon', 'night']])\n",
    "result2 = logist_mod2.fit()\n",
    "result2.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0415395967924728, 1.0421647080651766, 1.047807619637706)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.0407), np.exp(0.0413), np.exp(0.0467)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">With a `0.05` $\\alpha$ `level`, the p_values shows a statistical significance of our `time_category` in predicting the conversion on our website.\n",
    ">\n",
    ">We can interprete our coefficients as;\n",
    ">* Late in the morning, users are 1.04 times more likely to convert than early in the morning.\n",
    ">* In the aternoon, users are 1.04 times more likely to convert than early in the morning.\n",
    ">* At night, users are 1.05 more likely to convert than early in the morning.\n",
    ">\n",
    ">This seems reasonable as `early_mornings` are the about the time most people are sleepy and would tend not to want to engage more with activities and the `night` time is about the time when people engage more with online activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Using sklearn to test my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.0\n",
      "\n",
      "Recall score: 0.0\n",
      "\n",
      "Accuracy score: 0.8782080026696422\n",
      "\n",
      "Confusion matrix: \n",
      "[[84214     0]\n",
      " [11679     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test split of my data\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(df2[['late_morning', 'afternoon', 'night']], df2.converted, test_size=0.33)\n",
    "\n",
    "# Build and fit model\n",
    "sk_log_mod = LogisticRegression()\n",
    "sk_log_mod.fit(Xtrain, ytrain)\n",
    "\n",
    "# Predict with model\n",
    "ypred = sk_log_mod.predict(Xtest)\n",
    "\n",
    "# Compute metrices\n",
    "print('Precision score: {}\\n'.format(precision_score(ytest, ypred)))\n",
    "print('Recall score: {}\\n'.format(recall_score(ytest, ypred)))\n",
    "print('Accuracy score: {}\\n'.format(accuracy_score(ytest, ypred)))\n",
    "print('Confusion matrix: \\n{}'.format(confusion_matrix(ytest, ypred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives in. You will need to read in the **countries.csv** dataset and merge together your datasets on the appropriate rows.  [Here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) are the docs for joining tables. \n",
    "\n",
    "Does it appear that country had an impact on conversion?  Don't forget to create dummy variables for these country columns - **Hint: You will need two columns for the three dummy variables.** Provide the statistical output as well as a written response to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the country dataset\n",
    "df_country = pd.read_csv('countries.csv')\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US    203619\n",
       "UK     72466\n",
       "CA     14499\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>intercept</th>\n",
       "      <th>time_category</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>early_morning</th>\n",
       "      <th>late_morning</th>\n",
       "      <th>night</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>late_morning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>early_morning</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  timestamp      group landing_page  converted  \\\n",
       "0   851104 2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228 2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590 2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541 2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975 2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   ab_page  intercept  time_category  afternoon  early_morning  late_morning  \\\n",
       "0        0          1          night          0              0             0   \n",
       "1        0          1   late_morning          0              0             1   \n",
       "2        1          1      afternoon          1              0             0   \n",
       "3        1          1          night          0              0             0   \n",
       "4        0          1  early_morning          0              1             0   \n",
       "\n",
       "   night country  CA  UK  US  \n",
       "0      1      US   0   0   1  \n",
       "1      0      US   0   0   1  \n",
       "2      0      US   0   0   1  \n",
       "3      1      US   0   0   1  \n",
       "4      0      US   0   0   1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge country to df2\n",
    "df2 = df2.join(df_country.set_index('user_id'), on='user_id')\n",
    "\n",
    "# Creating and joining dummy variables for country column to df2\n",
    "df2 = df2.join(pd.get_dummies(df2.country))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366116\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-09-23 15:18</td>       <td>AIC:</td>        <td>212780.8333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212812.5723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>2</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290581</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9868</td>  <td>0.0114</td>  <td>-174.1736</td> <td>0.0000</td> <td>-2.0092</td> <td>-1.9645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>-0.0507</td>  <td>0.0284</td>   <td>-1.7863</td>  <td>0.0740</td> <td>-0.1064</td> <td>0.0049</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>-0.0099</td>  <td>0.0133</td>   <td>-0.7458</td>  <td>0.4558</td> <td>-0.0360</td> <td>0.0161</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-09-23 15:18 AIC:              212780.8333\n",
       "No. Observations:   290584           BIC:              212812.5723\n",
       "Df Model:           2                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290581           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9868    0.0114  -174.1736  0.0000  -2.0092  -1.9645\n",
       "CA           -0.0507    0.0284    -1.7863  0.0740  -0.1064   0.0049\n",
       "US           -0.0099    0.0133    -0.7458  0.4558  -0.0360   0.0161\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we build and fit our model\n",
    "logist_mod3 = sm.Logit(df2.converted, df2[['intercept', 'CA', 'US']])\n",
    "result3 = logist_mod3.fit()\n",
    "result3.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">On a **0.05** $\\alpha$ **level**, country has no statistical significance in predicting webpage conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though you have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if there significant effects on conversion.  Create the necessary additional columns, and fit the new model.  \n",
    "\n",
    "Provide the summary results, and your conclusions based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-09-23 15:18</td>       <td>AIC:</td>        <td>212782.6602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212846.1381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290578</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>-1.9922</td>  <td>0.0161</td>  <td>-123.4571</td> <td>0.0000</td> <td>-2.0238</td> <td>-1.9606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>0.0108</td>   <td>0.0228</td>   <td>0.4749</td>   <td>0.6349</td> <td>-0.0339</td> <td>0.0555</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>         <td>-0.0118</td>  <td>0.0398</td>   <td>-0.2957</td>  <td>0.7674</td> <td>-0.0899</td> <td>0.0663</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>         <td>0.0057</td>   <td>0.0188</td>   <td>0.3057</td>   <td>0.7598</td> <td>-0.0311</td> <td>0.0426</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA_ab_page</th> <td>-0.0783</td>  <td>0.0568</td>   <td>-1.3783</td>  <td>0.1681</td> <td>-0.1896</td> <td>0.0330</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US_ab_page</th> <td>-0.0314</td>  <td>0.0266</td>   <td>-1.1807</td>  <td>0.2377</td> <td>-0.0835</td> <td>0.0207</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-09-23 15:18 AIC:              212782.6602\n",
       "No. Observations:   290584           BIC:              212846.1381\n",
       "Df Model:           5                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290578           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9922    0.0161  -123.4571  0.0000  -2.0238  -1.9606\n",
       "ab_page       0.0108    0.0228     0.4749  0.6349  -0.0339   0.0555\n",
       "CA           -0.0118    0.0398    -0.2957  0.7674  -0.0899   0.0663\n",
       "US            0.0057    0.0188     0.3057  0.7598  -0.0311   0.0426\n",
       "CA_ab_page   -0.0783    0.0568    -1.3783  0.1681  -0.1896   0.0330\n",
       "US_ab_page   -0.0314    0.0266    -1.1807  0.2377  -0.0835   0.0207\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the interaction between country and ab_page.\n",
    "\n",
    "# Creating columns for interaction between CA, US and ab_page\n",
    "df2['CA_ab_page'] = df2['CA'] * df2['ab_page']\n",
    "df2['US_ab_page'] = df2['US'] * df2['ab_page']\n",
    "\n",
    "# Fitting model with interaction columns\n",
    "logist_mod4 = sm.Logit(df2.converted, df2[['intercept', 'ab_page', 'CA', 'US', 'CA_ab_page', 'US_ab_page']])\n",
    "result4 = logist_mod4.fit()\n",
    "result4.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">On a **0.05** $\\alpha$ **level**, `country` and `ab_page` interaction don't seem to have statistical significance in predicting conversion on our price. In order words they aren't very useful or don't lead to more conversion in our `treatment/new_page`.\n",
    "\n",
    "* Let's have a look at the interaction between time of day and `ab_page`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366092\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-09-23 15:18</td>       <td>AIC:</td>        <td>212776.8473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212861.4845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>7</td>         <td>Log-Likelihood:</td>  <td>-1.0638e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290576</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>           <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>         <td>-2.0098</td>  <td>0.0163</td>  <td>-123.5009</td> <td>0.0000</td> <td>-2.0417</td> <td>-1.9779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>           <td>-0.0377</td>  <td>0.0232</td>   <td>-1.6291</td>  <td>0.1033</td> <td>-0.0831</td> <td>0.0077</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>late_morning</th>      <td>0.0395</td>   <td>0.0228</td>   <td>1.7331</td>   <td>0.0831</td> <td>-0.0052</td> <td>0.0842</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>afternoon</th>         <td>0.0288</td>   <td>0.0229</td>   <td>1.2585</td>   <td>0.2082</td> <td>-0.0161</td> <td>0.0737</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>night</th>             <td>0.0154</td>   <td>0.0229</td>   <td>0.6727</td>   <td>0.5012</td> <td>-0.0295</td> <td>0.0603</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lmorning_ab_page</th>  <td>0.0022</td>   <td>0.0325</td>   <td>0.0668</td>   <td>0.9468</td> <td>-0.0615</td> <td>0.0659</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>afternoon_ab_page</th> <td>0.0252</td>   <td>0.0325</td>   <td>0.7768</td>   <td>0.4373</td> <td>-0.0384</td> <td>0.0889</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>night_ab_page</th>     <td>0.0628</td>   <td>0.0325</td>   <td>1.9354</td>   <td>0.0529</td> <td>-0.0008</td> <td>0.1264</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "===================================================================\n",
       "Model:               Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable:  converted        Pseudo R-squared: 0.000      \n",
       "Date:                2021-09-23 15:18 AIC:              212776.8473\n",
       "No. Observations:    290584           BIC:              212861.4845\n",
       "Df Model:            7                Log-Likelihood:   -1.0638e+05\n",
       "Df Residuals:        290576           LL-Null:          -1.0639e+05\n",
       "Converged:           1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "                   Coef.  Std.Err.     z     P>|z|   [0.025  0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept         -2.0098   0.0163 -123.5009 0.0000 -2.0417 -1.9779\n",
       "ab_page           -0.0377   0.0232   -1.6291 0.1033 -0.0831  0.0077\n",
       "late_morning       0.0395   0.0228    1.7331 0.0831 -0.0052  0.0842\n",
       "afternoon          0.0288   0.0229    1.2585 0.2082 -0.0161  0.0737\n",
       "night              0.0154   0.0229    0.6727 0.5012 -0.0295  0.0603\n",
       "lmorning_ab_page   0.0022   0.0325    0.0668 0.9468 -0.0615  0.0659\n",
       "afternoon_ab_page  0.0252   0.0325    0.7768 0.4373 -0.0384  0.0889\n",
       "night_ab_page      0.0628   0.0325    1.9354 0.0529 -0.0008  0.1264\n",
       "===================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interaction between ab_page and time of day.\n",
    "\n",
    "# Creating columns for interaction between CA, US and ab_page\n",
    "df2['lmorning_ab_page'] = df2['late_morning'] * df2['ab_page'] # lmorning -> For late_morning\n",
    "df2['afternoon_ab_page'] = df2['afternoon'] * df2['ab_page']\n",
    "df2['night_ab_page'] = df2['night'] * df2['ab_page']\n",
    "\n",
    "# Fitting model with interaction columns\n",
    "logist_mod5 = sm.Logit(df2.converted, df2[['intercept', 'ab_page', 'late_morning', 'afternoon', 'night', 'lmorning_ab_page', 'afternoon_ab_page', 'night_ab_page']])\n",
    "result5 = logist_mod5.fit()\n",
    "result5.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This interactions don't seem to be very useful in predicting conversion on our new_page. \n",
    ">\n",
    ">In general, the `new_page` don't seem to do better than the `old_page`, statistically based on conversion rates. It's best to stick with the `old_page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
